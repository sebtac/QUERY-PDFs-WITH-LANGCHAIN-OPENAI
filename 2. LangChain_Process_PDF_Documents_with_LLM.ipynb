{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xlorSbccWEDa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/sebtac/miniforge3/lib/python3.9/site-packages (0.0.275)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (2.8.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (2.3.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (1.21.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (0.0.27)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.11)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.4)\n",
      "Requirement already satisfied: openai in /Users/sebtac/miniforge3/lib/python3.9/site-packages (0.27.9)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: aiohttp in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.20->openai) (2022.6.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from PyPDF2) (4.7.1)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m565.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Requirement already satisfied: tiktoken in /Users/sebtac/miniforge3/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from tiktoken) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sebtac/miniforge3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2022.6.15)\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "### QUERY PDFs WITH LANGCHAIN & OPENAI\n",
    "######################################\n",
    "\n",
    "\"\"\"\n",
    "OBJECTIVE:\n",
    "- Explore options information retrival from text-based documents\n",
    "- Status: SUCCESS!\n",
    "    - the LANGCHAIN and OPENAI's GPT model was able to answer correctly a wide number of questions from the document\n",
    "    - it can be used to automate summarization of standardized documents\n",
    "        - dates, names, entities, key topics, sentiments, summarizations,...\n",
    "        - possibly, we can make assesement if a set of documents provide support or not for some case of interest\n",
    "        \n",
    "BASED ON:\n",
    "1.) https://www.udemy.com/course/langchain-guide-next-gen-chatgpt-llms-apps-with-langchain/learn/lecture/38797056#overview\n",
    "2.) https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a\n",
    "\n",
    "TESTS:\n",
    "1.) LLM's HALLUCINATION... provide question without the input document... and you still get an answer!?!??!\n",
    "\n",
    "2.) VECTOR SEARCH\n",
    "- document needs to be EMBEDED and put into a VECTOR DATABASE (ElasticVectorSearch, Pinecone, Weaviate, FAISS)\n",
    "- this allows to perform similarity search between embeddings of the docuemnt and that of the query.\n",
    "- test shows ability of the GPT model to answer a wide range of questions\n",
    "- it returns \"no information available\" if question asks about element not present in the document\n",
    "- we can also ask model to SUMMARIZE and asses SENTIMENT of the document with the PROMPT!!!\n",
    "\n",
    "3.) CHAIN-TYPES\n",
    "- key issue: base approach works with all data which can breach the input-rate-limits (1000-characters). \n",
    "    - thus you need to batch the input text into smaller chunks manually or use chain-types that do it automatically\n",
    "- available chain types: \"staff\", \"map_reduce\", \"refine\", \"map-rerank\"\n",
    "    - \"staff\" by default uses the whole document; all other chunk the data automatically\n",
    "        - detailes of each chain-type are BELOW\n",
    "- map_reduce seems to be the stronges possibly because it does use the whole information directly... although in chunks\n",
    "    - other approaches respondend not only with names of the authors of the paper but also with authors found in CITATIONS!?!!?\n",
    "    \n",
    "4.) RetrievalQA\n",
    "- A way to address the issue of working with ALL data. \n",
    "- it does initial seach for the most relevant chunks of the data!!! \n",
    "- and only those chunks are sent to OpenAI!!!\n",
    "- provided correct answer\n",
    "- two parameters that control the quality of answers\n",
    "    - search_type=\"mmr\", \"similarity\"\n",
    "    - search_kwargs={\"k\":2} # controls number of text segments to be exteacted and sent to OpenAI\n",
    "        - \"similarity\" required K=5 to produce consistently the correct answer\n",
    "        - \"mmr\" required K=2!!! - cheaper and faster to run!!!\n",
    "    \n",
    "5.) VectorstoreIndexCreator\n",
    "- wrapper on the above (4)\n",
    "- requires specific data loader which i had issues to install on my comp.\n",
    "- TBC...\n",
    "\n",
    "6.) ConversationalRetrievalChain\n",
    "- Like RetrievalQA, but allows to provide in the query the history of the discussion\n",
    "- it was able to distribuish between the author of the paper and the authors listed in CITATIONS!!!\n",
    "- Unfortunatelly, the example data does not provide interesting test case thus:\n",
    "- TBC...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "### ENVIRONMENT SETUP\n",
    "#####################\n",
    "\n",
    "#!pip install langchain\n",
    "#!pip install openai\n",
    "#!pip install PyPDF2\n",
    "#!pip install faiss-cpu # faiss-cpu, faiss-gpu\n",
    "#!pip install tiktoken\n",
    "\n",
    "# Get your API keys from openai, you will need to create an account. \n",
    "# Here is the link to get the keys: https://platform.openai.com/account/billing/overview\n",
    "\n",
    "#import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nq0vKGFeW1KD"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "### LOAD PACKAGES\n",
    "#################\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrcBhoIIeilJ"
   },
   "source": [
    "**Without data access**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "4-8LNCmHemvp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe authors of the article are Dr. David A. Sousa and Dr. Thomas R. Guskey.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "### LLM's HALLUCINATIONs!!! -- even though no doc was provided the model still generated a response!!!\n",
    "###########################\n",
    "\n",
    "template = \"\"\" {question}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"question\"], \n",
    "                                 template=template)\n",
    "llm = OpenAI(temperature=0.0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "chain.run(\"who are the authors of the article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L65AJ-afQ9_"
   },
   "source": [
    "**With access to data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "NalD3XkQWrJR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PyPDF2._reader.PdfReader object at 0x1773e2be0>\n",
      "raw_text\n",
      "\n",
      " GPT4All: Training an Assistant-style Chatbot with Large Scale Data\n",
      "Distillation from GPT-3.5-Turbo\n",
      "Yuvanesh Anand\n",
      "yuvanesh@nomic.aiZach Nussbaum\n",
      "zanussbaum@gmail.com\n",
      "Brandon Duderstadt\n",
      "brandon@nomic.aiBenjamin Schmidt\n",
      "ben@nomic.aiAndriy Mulyar\n",
      "andriy@nomic.ai\n",
      "Abstract\n",
      "This preliminary technical report describes the\n",
      "development of GPT4All, a chatbot trained\n",
      "over a massive curated corpus of assistant in-\n",
      "teractions including word problems, story de-\n",
      "scriptions, multi-turn dialogue, and code. We\n",
      "openly release the collected data, data cura-\n",
      "tion procedure, training code, and final model\n",
      "weights to promote open research and repro-\n",
      "ducibility. Additionally, we release quantized\n",
      "4-bit versions of the model allowing virtually\n",
      "anyone to run the model on CPU.\n",
      "1 Data Collection and Curation\n",
      "We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\n",
      "ple of questions/prompts by leveraging three pub-\n",
      "licly available datasets:\n",
      "• The unified chip2 subset of LAION OIG.\n",
      "• Coding questions with a random sub-sample\n",
      "of Stackoverflow Questions\n",
      "• Instruction-tuning with a sub-sample of Big-\n",
      "science/P3\n",
      "We chose to dedicate substantial attention to data\n",
      "preparation and curation based on commentary in\n",
      "the Stanford Alpaca project (Taori et al., 2023).\n",
      "Upon collection of the initial dataset of prompt-\n",
      "generation pairs, we loaded data into Atlas for data\n",
      "curation and cleaning. With Atlas, we removed all\n",
      "examples where GPT-3.5-Turbo failed to respond\n",
      "to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\n",
      "set from the final training dataset due to its very\n",
      "Figure 1: TSNE visualization of the candidate training\n",
      "data (Red: Stackoverflow, Orange: chip2, Blue: P3).\n",
      "The large blue balls (e.g. indicated by the red arrow)\n",
      "are highly homogeneous prompt-response pairs.\n",
      "low output diversity; P3 contains many homoge-\n",
      "neous prompts which produce short and homoge-\n",
      "neous responses from GPT-3.5-Turbo. This exclu-\n",
      "sion produces a final subset containing 437,605\n",
      "prompt-generation pairs, which is visualized in\n",
      "Figure 2. You can interactively explore the dataset\n",
      "at each stage of cleaning at the following links:\n",
      "• Cleaned with P3\n",
      "• Cleaned without P3 (Final Training Dataset)\n",
      "2 Model Training\n",
      "We train several models finetuned from an in-\n",
      "stance of LLaMA 7B (Touvron et al., 2023).\n",
      "The model associated with our initial public re-\n",
      "lease is trained with LoRA (Hu et al., 2021)\n",
      "on the 437,605 post-processed examples for four\n",
      "epochs. Detailed model hyper-parameters and\n",
      "training code can be found in the associated repos-\n",
      "itory and model training log.(a) TSNE visualization of the final training data, ten-colored\n",
      "by extracted topic.\n",
      "(b) Zoomed in view of Figure 2a. The region displayed con-\n",
      "tains generations related to personal health and wellness.\n",
      "Figure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\n",
      "2.1 Reproducibility\n",
      "We release all data (including unused P3 genera-\n",
      "tions), training code, and model weights for the\n",
      "community to build upon. Please check the Git\n",
      "repository for the most up-to-date data, training\n",
      "details and checkpoints.\n",
      "2.2 Costs\n",
      "We were able to produce these models with about\n",
      "four days work, $800 in GPU costs (rented from\n",
      "Lambda Labs and Paperspace) including several\n",
      "failed trains, and $500 in OpenAI API spend.\n",
      "Our released model, gpt4all-lora, can be trained in\n",
      "about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\n",
      "Instruct paper (Wang et al., 2022). We report the\n",
      "ground truth perplexity of our model against what\n",
      "is, to our knowledge, the best openly available\n",
      "alpaca-lora model, provided by user chainyo on\n",
      "huggingface. We find that all models have very\n",
      "large perplexities on a small number of tasks, and\n",
      "report perplexities clipped to a maximum of 100.\n",
      "Models finetuned on this collected dataset ex-\n",
      "hibit much lower perplexity in the Self-Instruct\n",
      "evaluation compared to Alpaca. This evaluation is\n",
      "in no way exhaustive and further evaluation work\n",
      "Figure 3: Model Perplexities. Lower is better. Our\n",
      "models achieve stochastically lower ground truth per-\n",
      "plexities than alpaca-lora.\n",
      "remains. We welcome the reader to run the model\n",
      "locally on CPU (see Github for files) and get a\n",
      "qualitative sense of what it can do.\n",
      "4 Use Considerations\n",
      "The authors release data and training details in\n",
      "hopes that it will accelerate open LLM research,\n",
      "particularly in the domains of alignment and inter-\n",
      "pretability. GPT4All model weights and data are\n",
      "intended and licensed only for research purposes\n",
      "and any commercial use is prohibited. GPT4All\n",
      "is based on LLaMA, which has a non-commercial\n",
      "license. The assistant data is gathered from Ope-\n",
      "nAI’s GPT-3.5-Turbo, whose terms of use pro-hibit developing models that compete commer-\n",
      "cially with OpenAI.\n",
      "References\n",
      "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\n",
      "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n",
      "Weizhu Chen. 2021. Lora: Low-rank adaptation of\n",
      "large language models.\n",
      "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n",
      "Dubois, Xuechen Li, Carlos Guestrin, Percy\n",
      "Liang, and Tatsunori B. Hashimoto. 2023. Stan-\n",
      "ford alpaca: An instruction-following llama\n",
      "model. https://github.com/tatsu-lab/\n",
      "stanford_alpaca .\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\n",
      "Baptiste Rozi `ere, Naman Goyal, Eric Hambro,\n",
      "Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\n",
      "Edouard Grave, and Guillaume Lample. 2023.\n",
      "Llama: Open and efficient foundation language\n",
      "models.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\n",
      "isa Liu, Noah A. Smith, Daniel Khashabi, and Han-\n",
      "naneh Hajishirzi. 2022. Self-instruct: Aligning lan-\n",
      "guage model with self generated instructions.\n",
      "raw_text[:100]\n",
      "\n",
      " GPT4All: Training an Assistant-style Chatbot with Large Scale Data\n",
      "Distillation from GPT-3.5-Turbo\n",
      "Y\n",
      "8 GPT4All: Training an Assistant-style Chatbot with Large Scale Data\n",
      "Distillation from GPT-3.5-Turbo\n",
      "Yuvanesh Anand\n",
      "yuvanesh@nomic.aiZach Nussbaum\n",
      "zanussbaum@gmail.com\n",
      "Brandon Duderstadt\n",
      "brandon@nomic.aiBenjamin Schmidt\n",
      "ben@nomic.aiAndriy Mulyar\n",
      "andriy@nomic.ai\n",
      "Abstract\n",
      "This preliminary technical report describes the\n",
      "development of GPT4All, a chatbot trained\n",
      "over a massive curated corpus of assistant in-\n",
      "teractions including word problems, story de-\n",
      "scriptions, multi-turn dialogue, and code. We\n",
      "openly release the collected data, data cura-\n",
      "tion procedure, training code, and final model\n",
      "weights to promote open research and repro-\n",
      "ducibility. Additionally, we release quantized\n",
      "4-bit versions of the model allowing virtually\n",
      "anyone to run the model on CPU.\n",
      "1 Data Collection and Curation\n",
      "We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam- We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\n",
      "ple of questions/prompts by leveraging three pub-\n",
      "licly available datasets:\n",
      "• The unified chip2 subset of LAION OIG.\n",
      "• Coding questions with a random sub-sample\n",
      "of Stackoverflow Questions\n",
      "• Instruction-tuning with a sub-sample of Big-\n",
      "science/P3\n",
      "We chose to dedicate substantial attention to data\n",
      "preparation and curation based on commentary in\n",
      "the Stanford Alpaca project (Taori et al., 2023).\n",
      "Upon collection of the initial dataset of prompt-\n",
      "generation pairs, we loaded data into Atlas for data\n",
      "curation and cleaning. With Atlas, we removed all\n",
      "examples where GPT-3.5-Turbo failed to respond\n",
      "to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "### DATA PREPARATION\n",
    "####################\n",
    "\n",
    "reader = PdfReader('2023_GPT4All_Technical_Report.pdf')\n",
    "print(reader)\n",
    "\n",
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "        \n",
    "print(\"raw_text\\n\\n\", raw_text)\n",
    "print(\"raw_text[:100]\\n\\n\", raw_text[:100])\n",
    "\n",
    "# We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits. \n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator = \"\\n\",\n",
    "                                      chunk_size = 1000,\n",
    "                                      chunk_overlap  = 200,\n",
    "                                      length_function = len)\n",
    "\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "\n",
    "print(len(texts), texts[0], texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "TcZUsQVyXBPX"
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "### SET EMBEDDINGS & VECTOR DATABSE\n",
    "###################################\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "wpQ2VnBvXI2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################\n",
    "### VECTOR SEARCH\n",
    "#################\n",
    "\n",
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")\n",
    "\n",
    "query = \"who are the authors of the article?\"\n",
    "\n",
    "docs = docsearch.similarity_search(query)\n",
    "\n",
    "\"\"\"\n",
    "for i in docs:\n",
    "    print(i,\"\\n\\n\")\n",
    "    \n",
    "print(docs[0], type(docs[0]), type(docs))\n",
    "\"\"\"\n",
    "\n",
    "#chain.run(input_documents=docs, question=query) # use ALL chunks\n",
    "chain.run(input_documents=[docs[3]], question=query) # use ONLY chunk with the information -- for some reason it was the last one>?!?!?!?\n",
    "\n",
    "queries = [\"What was the cost of training the GPT4all model?\",\n",
    "           \"How was the model trained?\",\n",
    "           \"what was the size of the training dataset?\",\n",
    "           \"How is this different from other models?\",\n",
    "           \"What is Google Bard?\",\n",
    "           \"Summarize the document content\",\n",
    "           \"What is the sentiment of the document?\",\n",
    "           \"Based on this paper, why should people avoid using AI?\"]\n",
    "\n",
    "for query in queries:\n",
    "    docs = docsearch.similarity_search(query)\n",
    "    result = chain.run(input_documents=docs, question=query)\n",
    "    print(\"Query:\", query, \"\\n\", \"Result:\", result, \"\\n\\n\")\n",
    "    time.sleep(20) # due to the query rate limits in OpenAI -- makes clearer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training\\nWe train several models finetuned from an in-\\nstance of LLaMA 7B (Touvron et al., 2023).\\nThe model associated with our initial public re-\\nlease is trained with LoRA (Hu et al., 2021)\\non the 437,605 post-processed examples for four\\nepochs. Detailed model hyper-parameters and\\ntraining code can be found in the associated repos-\\nitory and model training log.' metadata={'source': '2023_GPT4All_Technical_Report.pdf', 'page': 0} \n",
      "\n",
      "\n",
      "page_content='(a) TSNE visualization of the final training data, ten-colored\\nby extracted topic.\\n(b) Zoomed in view of Figure 2a. The region displayed con-\\ntains generations related to personal health and wellness.\\nFigure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\\n2.1 Reproducibility\\nWe release all data (including unused P3 genera-\\ntions), training code, and model weights for the\\ncommunity to build upon. Please check the Git\\nrepository for the most up-to-date data, training\\ndetails and checkpoints.\\n2.2 Costs\\nWe were able to produce these models with about\\nfour days work, $800 in GPU costs (rented from\\nLambda Labs and Paperspace) including several\\nfailed trains, and $500 in OpenAI API spend.\\nOur released model, gpt4all-lora, can be trained in\\nabout eight hours on a Lambda Labs DGX A100\\n8x 80GB for a total cost of $100 .\\n3 Evaluation\\nWe perform a preliminary evaluation of our model\\nusing the human evaluation data from the Self-\\nInstruct paper (Wang et al., 2022). We report the\\nground truth perplexity of our model against what\\nis, to our knowledge, the best openly available\\nalpaca-lora model, provided by user chainyo on\\nhuggingface. We find that all models have very\\nlarge perplexities on a small number of tasks, and\\nreport perplexities clipped to a maximum of 100.\\nModels finetuned on this collected dataset ex-\\nhibit much lower perplexity in the Self-Instruct\\nevaluation compared to Alpaca. This evaluation is\\nin no way exhaustive and further evaluation work\\nFigure 3: Model Perplexities. Lower is better. Our\\nmodels achieve stochastically lower ground truth per-\\nplexities than alpaca-lora.\\nremains. We welcome the reader to run the model\\nlocally on CPU (see Github for files) and get a\\nqualitative sense of what it can do.\\n4 Use Considerations\\nThe authors release data and training details in\\nhopes that it will accelerate open LLM research,\\nparticularly in the domains of alignment and inter-\\npretability. GPT4All model weights and data are\\nintended and licensed only for research purposes\\nand any commercial use is prohibited. GPT4All\\nis based on LLaMA, which has a non-commercial\\nlicense. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-' metadata={'source': '2023_GPT4All_Technical_Report.pdf', 'page': 1} \n",
      "\n",
      "\n",
      "page_content='hibit developing models that compete commer-\\ncially with OpenAI.\\nReferences\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\\nford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\\nguage model with self generated instructions.' metadata={'source': '2023_GPT4All_Technical_Report.pdf', 'page': 2} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "### CHAIN-TYPES\n",
    "###############\n",
    "\n",
    "# USE ALTERNATIVE DATA LODER -- this one does data-chunking automatically!\n",
    "\n",
    "#! pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"2023_GPT4All_Technical_Report.pdf\")\n",
    "#loader = PdfReader(\"2023_GPT4All_Technical_Report.pdf\") # EACH LOADER HAS DIFFERENT BEHAVIOUR AND SET OF ASSOCiATED FUCNTIONS\n",
    "documents = loader.load() # IT LOADs AND GROUPS INTO CHUNKS!!!\n",
    "\n",
    "# CHECK THE DOCs\n",
    "for i in documents:\n",
    "    print(i, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchain_type=\"stuff\" <<DEFAULT>> \\n- uses ALL of the text from the documents in the prompt. \\n- Might run into the MAX_LENGTH of the prompt error \\n \\n\"map_reduce: It separates texts into batches (as an example, you can define batch size in llm=OpenAI(batch_size=5)), feeds each batch with the question to LLM separately, and comes up with the final answer based on the answers from each batch.\\nrefine : It separates texts into batches, feeds the first batch to LLM, and feeds the answer and the second batch to LLM. It refines the answer by going through all the batches.\\nmap-rerank: It separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the high-scored answers from each batch.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "### CHAIN TYPES - DESCRIPTION:\n",
    "##############################\n",
    "\n",
    "\"\"\"\n",
    "-- GENRAL ISSUE: all methods ultimately use all data... possibly chanked but ALL\n",
    "\n",
    "chain_type=\"stuff\" <<DEFAULT>>:\n",
    "- uses ALL of the text from the documents in the prompt. \n",
    "- Might run into the MAX_LENGTH of the prompt error \n",
    " \n",
    "\"map_reduce\":\n",
    "- It separates texts into batches (as an example, you can define batch size in llm=OpenAI(batch_size=5)),\n",
    "- feeds each batch with the question to LLM separately - final answer based on the answers from each batch.\n",
    "\n",
    "\"refine\":\n",
    "It separates texts into batches, feeds the first batch to LLM, and feeds the answer and the second batch to LLM.\n",
    "It refines the answer by going through all the batches.\n",
    "\n",
    "\"map-rerank\":\n",
    "It separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question,\n",
    "and comes up with the final answer based on the high-scored answers from each batch.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_reduce\n",
      "response map_reduce :  The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.\n",
      "refine\n",
      "response refine : \n",
      "\n",
      "The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar, Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample, Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\n",
      "map-rerank\n",
      "SOME ERROR -- Possibly Rate Limit\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### CHAIN TYPES - COMPARISONs:\n",
    "##############################\n",
    "\n",
    "#chain_types = [\"stuff\", \"map_reduce\", \"refine\", \"map-rerank\"] # \"stuff\" -- LIMIT ERROR!!!\n",
    "chain_types = [\"map_reduce\", \"refine\", \"map-rerank\"]\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import time\n",
    "\n",
    "for chain_type in chain_types:\n",
    "    try:\n",
    "        print(chain_type)\n",
    "        chain = load_qa_chain(llm=OpenAI(), chain_type=chain_type)\n",
    "        query = \"who are the authors of the article?\"\n",
    "        response = chain.run(input_documents = documents, question = query)\n",
    "        print(\"response\", chain_type, \":\", response)\n",
    "    except:\n",
    "        print(\"SOME ERROR -- Possibly Rate Limit\")\n",
    "        pass\n",
    "    time.sleep(60)      \n",
    "\n",
    "# THIS LISTS ALL PEOPLE IN THE TEXT allso those in the citations!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### RESULTS of CHAIN_TYPES comparison:\n",
    "######################################\n",
    "\n",
    "\"\"\"\n",
    "response EXPECTED:\n",
    "- \"Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar\"\n",
    "\n",
    "response \"staff\":\n",
    "- works with all data in the document thus runs into input-length-limit\n",
    "\n",
    "response \"map_reduce\": -- listed also CITATIONS in the first run but not in the subsequent ones?!?!?!?!?!?\n",
    "- The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.\n",
    "\n",
    "response \"refine\": -- lists also CITATIONS!!!.. also in subsequent ones...\n",
    "- The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar...\n",
    "- , Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample, Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\n",
    "\n",
    "rsponse \"map-rerank\": -- lists also CITATIONS!!!.. also in subsequent ones...\n",
    "- The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar\n",
    "- , Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample, Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### For multiple documents\n",
    "##########################\n",
    "\n",
    "loaders = [....]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-g0rg9a7HAzehrPHro1S6kMLk on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-g0rg9a7HAzehrPHro1S6kMLk on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'who are the authors of the article?',\n",
       " 'result': ' The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############\n",
    "### RetrievalQA\n",
    "###############\n",
    "\n",
    "\"\"\"\n",
    "- this addresses the issue of working with ALL data. \n",
    "- it does initial seach for the most relevant chunks of the data!!! \n",
    "- and only those chunks are sent to OpenAI!!!\n",
    "\"\"\"\n",
    "#!pip install chromadb\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# create the vectorestore to use as the index\n",
    "#db = Chroma.from_documents(texts, embeddings)\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":2}) #\"mmr\", \"similarity\"\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=retriever, \n",
    "                                 return_source_documents=False) # True\n",
    "query = \"who are the authors of the article?\"\n",
    "result = qa({\"query\": query})\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### RetrievalQA RESULTS\n",
    "#######################\n",
    "\n",
    "# \"search_type=\"similarity\", search_kwargs={\"k\":2} -- WEAK - shows only soem citations not the authors of the paper!!!\n",
    "\n",
    "\"\"\"\n",
    "{'query': 'who are the authors of the article?',\n",
    " 'result': ' The authors of the article are Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.'}\n",
    " \"\"\"\n",
    "\n",
    "# search_type=\"similarity\", search_kwargs={\"k\":5} -- CORRECT RESULTS!!!\n",
    "\n",
    "{'query': 'who are the authors of the article?',\n",
    " 'result': ' The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.'}\n",
    "\n",
    "\n",
    "# search_type=\"mmr\", search_kwargs={\"k\":2} -- CORRECT RESULTS!!!\n",
    "\n",
    "{'query': 'who are the authors of the article?',\n",
    " 'result': ' The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticImportError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/chromadb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtelemetry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientStartEvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/chromadb/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pydantic/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pydantic/_migration.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mPydanticImportError\u001b[0m: `BaseSettings` has been moved to the `pydantic-settings` package. See https://docs.pydantic.dev/2.3/migration/#basesettings-has-moved-to-pydantic-settings for more details.\n\nFor further information visit https://errors.pydantic.dev/2.3/u/import-error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/d_xxpqw56t95qlbw1w07l3340000gn/T/ipykernel_11089/2407996726.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorstoreIndexCreator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorstoreIndexCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#index = VectorstoreIndexCreator().from_documents([docsearch])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"who are the authors of the article?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/indexes/vectorstore.py\u001b[0m in \u001b[0;36mfrom_loaders\u001b[0;34m(self, loaders)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVectorStoreIndexWrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/indexes/vectorstore.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;34m\"\"\"Create a vectorstore index from documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0msub_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         vectorstore = self.vectorstore_cls.from_documents(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0msub_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstore_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         )\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChroma\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \"\"\"\n\u001b[0;32m--> 567\u001b[0;31m         chroma_collection = cls(\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;34m\"Could not import chromadb python package. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34m\"Please install it with `pip install chromadb`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "### Method 3: VectorstoreIndexCreator\n",
    "#####################################\n",
    "\n",
    "# REQUIRES CHROMADB, which i cannot install on my MAC!!!!!\n",
    "#! pip install chromadb, pydantic-settings\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "#index = VectorstoreIndexCreator().from_documents([docsearch])\n",
    "query = \"who are the authors of the article?\"\n",
    "index.query(llm=OpenAI(), chain_type=\"stuff\", question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'who are the authors of the article?',\n",
       " 'chat_history': [],\n",
       " 'answer': ' The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'who are the authors of the citations in the article?',\n",
       " 'chat_history': [('who are the authors of the article?',\n",
       "   ' The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.')],\n",
       " 'answer': ' Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, and Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################\n",
    "### ConversationalRetrievalChain\n",
    "################################\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# create the vectorestore to use as the index\n",
    "#db = Chroma.from_documents(texts, embeddings)\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":2}) #\"mmr\", \"similarity\"\n",
    "# create a chain to answer questions \n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(), retriever)\n",
    "\n",
    "chat_history = []\n",
    "query = \"who are the authors of the article?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "display(result)\n",
    "\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"who are the authors of the citations in the article?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings, \n",
    "embeddings.Config(), \"\\n\\n\",\n",
    "embeddings.allowed_special, \"\\n\\n\",\n",
    "embeddings.chunk_size, \"\\n\\n\",\n",
    "embeddings.disallowed_special, \"\\n\\n\",\n",
    "embeddings.embed_documents, \"\\n\\n\",\n",
    "embeddings.embed_query, \"\\n\\n\",\n",
    "embeddings.embedding_ctx_length, \"\\n\\n\",\n",
    "embeddings.headers, \"\\n\\n\",\n",
    "embeddings.model, \"\\n\\n\",\n",
    "embeddings.model_kwargs, \"\\n\\n\",\n",
    "embeddings.schema(), \"\\n\\n\",\n",
    "embeddings.schema_json(), \"\\n\\n\",\n",
    "embeddings.tiktoken_model_name, \"\\n\\n\",\n",
    "embeddings.validate, \"\\n\\n\",\n",
    "embeddings.validate_environment, \"\\n\\n\",\n",
    "     )\n",
    "\n",
    "print(dir(embeddings))\n",
    "\n",
    "print(docsearch, docsearch.docstore, dir(docsearch))\n",
    "\n",
    "print(dir(docsearch.docstore), docsearch.docstore.search(\"who is the author?\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
